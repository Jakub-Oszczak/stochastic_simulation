{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_interval(samples, conf_level=0.95):\n",
    "    n_samples = len(samples)\n",
    "    sample_mean = np.mean(samples)\n",
    "    sample_std_dev = np.std(samples, ddof=1)\n",
    "    \n",
    "    # Calculate the margin of error\n",
    "    margin_error = stats.t.ppf((1 + conf_level) / 2, n_samples - 1) * sample_std_dev / np.sqrt(n_samples)\n",
    "    \n",
    "    # Compute the confidence interval bounds\n",
    "    conf_interval = (sample_mean - margin_error, sample_mean + margin_error)\n",
    "    \n",
    "    return sample_mean, conf_interval\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Step 1: Estimate integral using crude Monte Carlo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated variance: 0.2688\n",
      "Integral estimate: 1.7103\n",
      "95% confidence interval: (1.6069, 1.8137)\n",
      "Exact integral value: 1.7183\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 random numbers uniformly distributed between 0 and 1\n",
    "random_uniform_samples = np.random.uniform(0, 1, 100)\n",
    "exp_values = np.exp(random_uniform_samples)\n",
    "\n",
    "# Calculate and print the variance of the exponential values\n",
    "variance_estimate_1 = np.var(exp_values)\n",
    "print('Estimated variance: {:.4f}'.format(variance_estimate_1))\n",
    "\n",
    "# Calculate the confidence interval using the new function\n",
    "mean_estimate_1, (conf_lower, conf_upper) = compute_confidence_interval(exp_values)\n",
    "\n",
    "# Output the results\n",
    "print('Integral estimate: {:.4f}'.format(mean_estimate_1))\n",
    "print('95% confidence interval: ({:.4f}, {:.4f})'.format(conf_lower, conf_upper))\n",
    "print('Exact integral value: {:.4f}'.format(np.e - 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Step 2: Estimate integral using antithetic variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated variance: 0.0041\n",
      "Integral estimate: 1.7173\n",
      "95% confidence interval: (1.7046, 1.7300)\n",
      "Exact integral value: 1.7183\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 random numbers uniformly distributed between 0 and 1\n",
    "random_uniform_samples = np.random.uniform(0, 1, 100)\n",
    "exp_values = np.exp(random_uniform_samples)\n",
    "\n",
    "# Compute antithetic pairs and average\n",
    "y = (exp_values + np.exp(1 - random_uniform_samples)) / 2\n",
    "\n",
    "# Calculate and print the variance of the combined values\n",
    "variance_estimate_2 = np.var(y)\n",
    "print('Estimated variance: {:.4f}'.format(variance_estimate_2))\n",
    "\n",
    "# Calculate the confidence interval using the antithetic variables\n",
    "mean_estimate_2, (conf_lower, conf_upper) = compute_confidence_interval(y)\n",
    "\n",
    "# Output the results\n",
    "print('Integral estimate: {:.4f}'.format(mean_estimate_2))\n",
    "print('95% confidence interval: ({:.4f}, {:.4f})'.format(conf_lower, conf_upper))\n",
    "print('Exact integral value: {:.4f}'.format(np.e - 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Step 3: Estimate integral using control variates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated variance: 0.0035\n",
      "Integral estimate: 1.7186\n",
      "95% confidence interval: (1.7069, 1.7303)\n",
      "Exact integral value: 1.7183\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 random numbers uniformly distributed between 0 and 1\n",
    "random_uniform_samples = np.random.uniform(0, 1, 100)\n",
    "exp_values = np.exp(random_uniform_samples)\n",
    "\n",
    "# Calculate the coefficient c using control variates\n",
    "c = -np.cov(exp_values, random_uniform_samples)[0, 1] / np.var(random_uniform_samples)\n",
    "\n",
    "# Adjust the values using the control variate\n",
    "adjusted_values = exp_values + c * (random_uniform_samples - 0.5)\n",
    "\n",
    "# Calculate and print the variance of the adjusted values\n",
    "variance_estimate_3 = np.var(adjusted_values)\n",
    "print('Estimated variance: {:.4f}'.format(variance_estimate_3))\n",
    "\n",
    "# Calculate the confidence interval using the control variate adjusted values\n",
    "mean_estimate_3, (conf_lower, conf_upper) = compute_confidence_interval(adjusted_values)\n",
    "\n",
    "# Output the results\n",
    "print('Integral estimate: {:.4f}'.format(mean_estimate_3))\n",
    "print('95% confidence interval: ({:.4f}, {:.4f})'.format(conf_lower, conf_upper))\n",
    "print('Exact integral value: {:.4f}'.format(np.e - 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Step 4: Estimate integral using stratified sampling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated variance: 0.2420\n",
      "Integral estimate: 1.7190\n",
      "95% confidence interval: (1.3481, 2.0899)\n",
      "Exact integral value: 1.7183\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified sampling\n",
    "num_strata = 10\n",
    "samples_per_stratum = 10\n",
    "n = num_strata * samples_per_stratum\n",
    "\n",
    "# Generate stratified samples and compute the exponential function\n",
    "strata_means = []\n",
    "for i in range(num_strata):\n",
    "    us = np.random.uniform(0, 1, samples_per_stratum)\n",
    "    stratum_samples = [np.exp((i + us[j]) / num_strata) for j in range(samples_per_stratum)]\n",
    "    strata_means.append(np.mean(stratum_samples))\n",
    "\n",
    "# Calculate and print the variance of the stratified sample means\n",
    "variance_estimate_4 = np.var(strata_means)\n",
    "print('Estimated variance: {:.4f}'.format(variance_estimate_4))\n",
    "\n",
    "# Calculate the confidence interval using the stratified sample means\n",
    "mean_estimate_4, (conf_lower, conf_upper) = compute_confidence_interval(strata_means)\n",
    "\n",
    "# Output the results\n",
    "print('Integral estimate: {:.4f}'.format(mean_estimate_4))\n",
    "print('95% confidence interval: ({:.4f}, {:.4f})'.format(conf_lower, conf_upper))\n",
    "print('Exact integral value: {:.4f}'.format(np.e - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can compare all of the estimated variances and mean estimates for all of the four methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Method  Mean Estimate  Variance Estimate\n",
      "0     Crude Monte Carlo       1.710308           0.268803\n",
      "1  Antithetic Variables       1.717276           0.004072\n",
      "2      Control Variates       1.718574           0.003456\n",
      "3   Stratified Sampling       1.718983           0.241966\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Placeholder data for demonstration\n",
    "data = {\n",
    "    \"Method\": [\"Crude Monte Carlo\", \"Antithetic Variables\", \"Control Variates\", \"Stratified Sampling\"],\n",
    "    \"Mean Estimate\": [mean_estimate_1, mean_estimate_2, mean_estimate_3, mean_estimate_4],\n",
    "    \"Variance Estimate\": [variance_estimate_1, variance_estimate_2, variance_estimate_3, variance_estimate_4]\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Displaying the table\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Results speak for themself. Two methods with the lowest variance are Antithetic Variables and Control Variates. Two methods with the highest variance are Crude Monte Carlo and Stratified Sampling*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Step 5: Control variates for blocking queueing system simulation*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_servers = 10\n",
    "avg_service_time = 8\n",
    "avg_interarrival_time = 1\n",
    "total_customers = 10000\n",
    "simulation_count = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function models the scenario as a Poisson process for customer arrivals coupled with exponential service durations. It outputs both the proportion of customers who couldn't be immediately served (blocked customers) and the average time between arrivals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queue_simulation(server_count, customer_count, service_time_avg, interarrival_time_avg):\n",
    "    times_service_ends = np.zeros(server_count)\n",
    "    current_time = 0\n",
    "    total_blocked = 0\n",
    "    total_arrival_time = 0\n",
    "    \n",
    "    for _ in range(customer_count):\n",
    "        next_arrival = stats.expon.rvs(scale=interarrival_time_avg)\n",
    "        total_arrival_time += next_arrival\n",
    "        current_time += next_arrival\n",
    "        next_service_end = times_service_ends.min()\n",
    "        available_server = times_service_ends.argmin()\n",
    "\n",
    "        if current_time < next_service_end:\n",
    "            total_blocked += 1\n",
    "        else:\n",
    "            service_duration = stats.expon.rvs(scale=service_time_avg)\n",
    "            times_service_ends[available_server] = current_time + service_duration\n",
    "\n",
    "    return total_blocked / customer_count, total_arrival_time / customer_count\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform ten simulations, saving for each one the fraction of blocked customers and the mean arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockage_rates = []\n",
    "mean_arrivals = []\n",
    "for _ in range(simulation_count):\n",
    "    blockage, arrival = queue_simulation(num_servers, total_customers, avg_service_time, avg_interarrival_time)\n",
    "    blockage_rates.append(blockage)\n",
    "    mean_arrivals.append(arrival)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The control variates technique is employed to increase the accuracy of our estimates by reducing their variance. This method uses additional, correlated variables to adjust the estimates, thereby refining the precision of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original blockage rate estimate: 0.1226\n",
      "Confidence interval: 0.1186, 0.1265\n",
      "\n",
      "Adjusted blockage rate estimate: 0.1201\n",
      "Confidence interval: 0.1174, 0.1228\n",
      "Variance of original estimates: 2.7202399999999996e-05\n",
      "Variance of adjusted estimates: 1.2814848292053273e-05\n"
     ]
    }
   ],
   "source": [
    "blockage_rates = np.array(blockage_rates)\n",
    "mean_arrivals = np.array(mean_arrivals)\n",
    "\n",
    "control_variable = -np.cov(blockage_rates, mean_arrivals)[0, 1] / mean_arrivals.var()\n",
    "adjusted_blockage_rates = blockage_rates + control_variable * (mean_arrivals - avg_interarrival_time)\n",
    "\n",
    "mean_blockage, (ci_lower, ci_upper) = compute_confidence_interval(blockage_rates)\n",
    "adjusted_mean_blockage, (adjusted_ci_lower, adjusted_ci_upper) = compute_confidence_interval(adjusted_blockage_rates)\n",
    "\n",
    "print(f'Original blockage rate estimate: {mean_blockage:.4f}')\n",
    "print(f'Confidence interval: {ci_lower:.4f}, {ci_upper:.4f}\\n')\n",
    "\n",
    "print(f'Adjusted blockage rate estimate: {adjusted_mean_blockage:.4f}')\n",
    "print(f'Confidence interval: {adjusted_ci_lower:.4f}, {adjusted_ci_upper:.4f}')\n",
    "\n",
    "print(f'Variance of original estimates: {blockage_rates.var()}')\n",
    "print(f'Variance of adjusted estimates: {adjusted_blockage_rates.var()}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that the variance has been reduced using control variates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Step 6: Common random numbers in queueing system simulation*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_servers = 10\n",
    "avg_service_time = 8\n",
    "avg_interarrival_time = 1\n",
    "total_customers = 10000\n",
    "simulation_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_queue_q2(nserver, customers, mean_st, mean_tbc, type='Exp', seed=0):\n",
    "    np.random.seed(seed)\n",
    "    if type == 'Exp':\n",
    "        arrival_times = stats.expon(scale=1/mean_tbc).rvs(size=customers)\n",
    "    elif type == 'Hyp':\n",
    "        # Generate hyperexponential distribution\n",
    "        p = 0.8\n",
    "        rate1, rate2 = 0.8333, 5\n",
    "        uniform_randoms = np.random.rand(customers)\n",
    "        exp1 = stats.expon(scale=1/rate1).rvs(size=customers)\n",
    "        exp2 = stats.expon(scale=1/rate2).rvs(size=customers)\n",
    "        arrival_times = np.where(uniform_randoms < p, exp1, exp2)\n",
    "    \n",
    "    server_time = np.zeros(nserver)\n",
    "    time = 0\n",
    "    blocked = 0\n",
    "\n",
    "    for i in range(customers):\n",
    "        delta_arrival_time = arrival_times[i]\n",
    "        time += delta_arrival_time\n",
    "        min_server = np.min(server_time)\n",
    "        idx_min_server = np.argmin(server_time)\n",
    "        if time < min_server:\n",
    "            blocked += 1\n",
    "        else:\n",
    "            server_time[idx_min_server] = time + stats.expon(scale=mean_st).rvs()\n",
    "\n",
    "    return blocked / customers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform ten simulations of the two processes using ten different seeds, saving the blocked fraction of customers for each run. Then we perform a paired t-test to see if there is a difference between the two processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 3.3864360266556224, P-value: 0.008044434371616256\n",
      "Estimated difference between the two processes: 0.0136\n"
     ]
    }
   ],
   "source": [
    "runs = []\n",
    "for i in range(simulation_count):\n",
    "    blocked_hyp = simulate_queue_q2(num_servers, total_customers, avg_service_time, avg_interarrival_time, 'Hyp', i)\n",
    "    blocked_exp = simulate_queue_q2(num_servers, total_customers, avg_service_time, avg_interarrival_time, 'Exp', i)\n",
    "    runs.append([blocked_hyp, blocked_exp])\n",
    "\n",
    "runs = np.array(runs)\n",
    "t_stat, p_value = stats.ttest_rel(runs[:, 0], runs[:, 1])\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
    "print(f'Estimated difference between the two processes: {np.mean(runs[:, 0] - runs[:, 1]):.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is very low, indicating a strong suggestion that the two processes perform differently. The estimation of the difference in performance is almost 2%, as printed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Monte Carlo and importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to estimate the probability of Z>a by generating 10 000 values from normal distribution and verifying how many of those values are greater than a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated probability of Z>a: 0.00000000\n",
      "The true value of Z>a probability: 0.00003167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = 4\n",
    "num_of_samples = 10_000\n",
    "\n",
    "samples = np.random.randn(num_of_samples)\n",
    "samples = samples > a\n",
    "mean = np.mean(samples)\n",
    "true_value = 1 - stats.norm.cdf(a)\n",
    "\n",
    "print('''\n",
    "Estimated probability of Z>a: {:.8f}\n",
    "The true value of Z>a probability: {:.8f}\n",
    "'''.format(mean, true_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a=2 was used the estimation of the probability that Z>2 was more accurate since value a=2 lays reasonably near the mean of normal distribution, but with a=4 used the estimation was much worse. It is due to a=4 being much more extreme value so not many samples in those regions are generated. It is needed to significantly raise the number of samples drawn, to increase the accuracy of estimation. Now let's use importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated probability of Z>a: 0.00003097\n",
      "The true value of Z>a probability: 0.00003167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "variance = 1\n",
    "normal_samples = stats.norm.rvs(loc = a, scale = variance, size = num_of_samples)\n",
    "\n",
    "target_density = stats.norm.pdf(normal_samples)\n",
    "proposal_density = stats.norm.pdf(normal_samples, loc = a, scale = variance)\n",
    "weights = target_density/proposal_density * (normal_samples > a)\n",
    "\n",
    "mean = np.mean(weights)\n",
    "true_value = 1 - stats.norm.cdf(a)\n",
    "\n",
    "print('''\n",
    "Estimated probability of Z>a: {:.8f}\n",
    "The true value of Z>a probability: {:.8f}\n",
    "'''.format(mean, true_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the estimated proability is rather more accurate. Previously it was usually 0 or close to 0. Now its also very low since a=4 is an extreme value, but it doesn't equal 0 anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Importance Sampling - exponential distribution and integral calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we will try to find the near optimal value of lambda by iteratively calculating the variance of h(x)f(x)/g(x) for the variety of lambda values. Next we will calculate the estimation of the integral of exponential function from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda used: 1.32653061\n",
      "      \n",
      "Estimated value of the integral: 1.70950722\n",
      "Estimated variance: 3.11402401\n",
      "The true value of the integral: 1.71828183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_values = np.linspace(0.5, 2.0, 50)\n",
    "num_of_samples = 10_000\n",
    "best_lambda = None\n",
    "min_variance = float('inf')\n",
    "results = []\n",
    "\n",
    "def estimate_integral(λ, sample_size):\n",
    "    samples = stats.expon.rvs(scale=1/λ, size=sample_size)\n",
    "    mask = (samples >= 0) & (samples <= 1)\n",
    "    function_values = np.exp(samples)\n",
    "    proposal_pdf = λ * np.exp(-λ * samples)\n",
    "    weights = mask * function_values / proposal_pdf\n",
    "    estimated_mean = np.mean(weights)\n",
    "    variance = np.var(weights, ddof=1)\n",
    "    return estimated_mean, variance\n",
    "\n",
    "for λ in lambda_values:\n",
    "    mean, variance = estimate_integral(λ, num_of_samples)\n",
    "    results.append((λ, mean, variance))\n",
    "    if variance < min_variance:\n",
    "        min_variance = variance\n",
    "        best_lambda = λ\n",
    "\n",
    "best_mean, best_variance = estimate_integral(best_lambda, num_of_samples)\n",
    "\n",
    "print('''\n",
    "Lambda used: {:.8f}\n",
    "      \n",
    "Estimated value of the integral: {:.8f}\n",
    "Estimated variance: {:.8f}\n",
    "The true value of the integral: {:.8f}\n",
    "'''.format(best_lambda, best_mean, best_variance, np.e - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closeness to True Value: The estimated value of the integral is relatively close to the true value, suggesting that the importance sampling method is effective, but the accuracy could potentially be improved.\n",
    "\n",
    "High Variance: The relatively high variance in the estimation suggests that the exponential sampling distribution might not be perfectly aligned with the integral. This misalignment could be causing inefficient sampling, leading to a higher variance in the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Pareto: IS estimator using the first moment distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have estimated a mean of Pareto distribution using importance sampling with First Moment Pareto Distribution as sampling distribution which is a Pareto distribution with \"k\" parameter decreased by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculated mean estimate: 40.99590000000016\n",
      "Confidence interval: lower: 40.98786317481932, upper: 41.00393682518101\n",
      "True mean value: 41.00000000000014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 1.025\n",
    "pareto_samples = stats.pareto.rvs(k - 1, size=num_of_samples)\n",
    "\n",
    "target_density = stats.pareto.pdf(pareto_samples, k)\n",
    "proposal_density = stats.pareto.pdf(pareto_samples, k - 1)\n",
    "\n",
    "importance_weights = pareto_samples * (target_density / proposal_density)\n",
    "\n",
    "estimated_mean, confidence_bounds = compute_confidence_interval(importance_weights)\n",
    "true_mean = k / (k - 1)\n",
    "\n",
    "print(f'''\n",
    "Calculated mean estimate: {estimated_mean}\n",
    "Confidence interval: lower: {confidence_bounds[0]}, upper: {confidence_bounds[1]}\n",
    "True mean value: {true_mean}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have achieved very precise estimation. Therefore yes, this approach is meaningul, it can reduce the variance of the estimator by using a sampling distribution g(x) that emphasizes areas contributing significantly to the expectation. This approach indeed can be generalized. Summing up, importance sampling has proven to be very effective by greatly reducing variance. It does this by focusing on sampling from areas that are most important for the estimate. For this method to give satisfactory results the distribution used for sampling has to be similar to the target distribution. The choice of the right g(x) function is crucial as it has significant impact on the methods performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
